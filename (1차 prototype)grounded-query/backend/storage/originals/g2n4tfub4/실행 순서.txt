  실행 순서
#chmod +x /home/lys/Documents/GitHub/Document-Management-Agent-Project/test/grounded-query/backend/start_vllm.sh

  1. (선택) 가상환경 생성

     cd /home/lys/Documents/GitHub/Document-Management-Agent-Project/test/grounded-query/backend
     python3 -m venv .venv && source .venv/bin/activate
     pip install -r requirements.txt
     cp .env.example .env
     필요 시 .env에서 VLLM_BASE_URL, VLLM_MODEL_NAME, FASTAPI_PORT 등을 조정하세요.
  2. 로컬 모델을 vLLM OpenAI 서버로 올리기

     pip install vllm  # 아직 없다면
     python3 -m vllm.entrypoints.openai.api_server \
       --model /home/lys/Desktop/qwen3_4b_ft/final_merged_model \
       --served-model-name qwen3-4b-ft \
       --host 127.0.0.1 --port 9000 \
       --dtype auto --tensor-parallel-size 1
     다른 포트를 쓰고 싶다면 .env의 VLLM_BASE_URL도 같이 바꿔 주세요.
  3. FastAPI 백엔드 기동

     cd /home/lys/Documents/GitHub/Document-Management-Agent-Project/test/grounded-query/backend
     source .venv/bin/activate
     uvicorn main:app --host 127.0.0.1 --port 8000 --reload
  4. 프런트엔드에서 백엔드 주소 지정(grounded-query/.env.local)

     VITE_FASTAPI_URL=http://127.0.0.1:8000
     이후 npm install && npm run dev로 UI를 켜면 업로드→임베딩→질의 흐름이 동작합니다.

  주의 및 다음 작업

  - 현재 파서가 pdf, docx, txt, md만 지원합니다(backend/main.py#L122). 추가 확장자가 필요하면 이 부분을 보강해야 합니다.
  - FastAPI /documents/{doc_id} 삭제 엔드포인트는 프런트에서 바로 호출하지만, 이미 생성한 벡터 인덱스가 많다면 백그라운드 작업
    으로 바꾸는 것도 고려해 볼 수 있습니다.
  - 백엔드가 추출한 청크 텍스트를 그대로 돌려주므로, 필요하면 마스킹/후처리를 추가하세요.

